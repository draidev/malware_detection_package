"""
this file extracts the required information of a given file using the library PE 

"""
import pefile
import array
import math
import os
import joblib

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import category_encoders as ce

import numpy as np
import pandas as pd
from .data_loader import get_full_path

from imblearn.under_sampling import \
    RandomUnderSampler, TomekLinks, OneSidedSelection, CondensedNearestNeighbour, EditedNearestNeighbours

categorical_columns=['Machine', 'Characteristics', 'MajorLinkerVersion',
                     'MinorLinkerVersion', 'SectionAlignment', 'FileAlignment',
                     'Subsystem', 'MajorOperatingSystemVersion', 'MinorOperatingSystemVersion',
                     'MajorImageVersion', 'MinorImageVersion', 'MajorSubsystemVersion',
                     'MinorSubsystemVersion']

numerical_columns=['SizeOfCode','SizeOfInitializedData', 'SizeOfUninitializedData',
       'AddressOfEntryPoint', 'BaseOfCode', 'BaseOfData', 'ImageBase', 'SizeOfImage',
       'SizeOfHeaders', 'CheckSum', 'DllCharacteristics',
       'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve',
       'SizeOfHeapCommit', 'NumberOfRvaAndSizes', 'SectionsNb',
       'SectionsMeanEntropy', 'SectionsMinEntropy', 'SectionsMaxEntropy',
       'SectionsMeanRawsize', 'SectionsMinRawsize', 'SectionsMaxRawsize',
       'SectionsMeanVirtualsize', 'SectionsMinVirtualsize',
       'SectionMaxVirtualsize', 'ImportsNbDLL', 'ImportsNb',
       'ImportsNbOrdinal', 'ExportNb', 'ResourcesNb', 'ResourcesMeanEntropy',
       'ResourcesMinEntropy', 'ResourcesMaxEntropy', 'ResourcesMeanSize',
       'ResourcesMinSize', 'ResourcesMaxSize', 'LoadConfigurationSize',
       'VersionInformationSize']

ENCODER_SUFFIX = ".enc"
SCALER_SUFFIX = ".scl"
LABEL_SUFFIX = ".lb"




def get_entropy(data):
    if len(data) == 0:
        return 0.0
    occurences = array.array('L', [0]*256)
    for x in data:
        occurences[x if isinstance(x, int) else ord(x)] += 1
    entropy = 0
    for x in occurences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x*math.log(p_x, 2)
    return entropy


def get_resources(pe):
    """Extract resources :
    [entropy, size]"""
    resources = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                resources.append([entropy, size])
        except Exception as e:
            return resources
    return resources


def get_version_info(pe):
    """Return version infos"""
    res = {}
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]
    if hasattr(pe, 'VS_FIXEDFILEINFO'):
        res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
        res['os'] = pe.VS_FIXEDFILEINFO.FileOS
        res['type'] = pe.VS_FIXEDFILEINFO.FileType
        res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
        res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
        res['signature'] = pe.VS_FIXEDFILEINFO.Signature
        res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion
    return res


#extract the info for a given file
def extract_infos(fpath):
    res = {}
    pe = pefile.PE(fpath)
    res['Machine'] = pe.FILE_HEADER.Machine
    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    res['Characteristics'] = pe.FILE_HEADER.Characteristics
    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    try:
        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
    except AttributeError:
        res['BaseOfData'] = 0
    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage
    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders
    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit
    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags
    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes
    
       # Sections
    res['SectionsNb'] = len(pe.sections)
    entropy = list(map(lambda x:x.get_entropy(), pe.sections))
    res['SectionsMeanEntropy'] = sum(entropy)/float(len(entropy))
    res['SectionsMinEntropy'] = min(entropy)
    res['SectionsMaxEntropy'] = max(entropy)
    raw_sizes = list(map(lambda x:x.SizeOfRawData, pe.sections))
    res['SectionsMeanRawsize'] = sum(raw_sizes)/float(len(raw_sizes))
    res['SectionsMinRawsize'] = min(raw_sizes)
    res['SectionsMaxRawsize'] = max(raw_sizes)
    virtual_sizes = list(map(lambda x:x.Misc_VirtualSize, pe.sections))
    res['SectionsMeanVirtualsize'] = sum(virtual_sizes)/float(len(virtual_sizes))
    res['SectionsMinVirtualsize'] = min(virtual_sizes)
    res['SectionMaxVirtualsize'] = max(virtual_sizes)

    #Imports
    try:
        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)
        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
        res['ImportsNb'] = len(imports)
        res['ImportsNbOrdinal'] = len(list(filter(lambda x:x.name is None, imports)))
    except AttributeError:
        res['ImportsNbDLL'] = 0
        res['ImportsNb'] = 0
        res['ImportsNbOrdinal'] = 0

    #Exports
    try:
        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
    except AttributeError:
        # No export
        res['ExportNb'] = 0
    #Resources
    resources= get_resources(pe)
    res['ResourcesNb'] = len(resources)
    
    if len(resources)> 0:
        entropy = list(map(lambda x:x[0], resources))
        res['ResourcesMeanEntropy'] = sum(entropy)/float(len(entropy))
        res['ResourcesMinEntropy'] = min(entropy)
        res['ResourcesMaxEntropy'] = max(entropy)
        sizes = list(map(lambda x:x[1], resources))
        res['ResourcesMeanSize'] = sum(sizes)/float(len(sizes))
        res['ResourcesMinSize'] = min(sizes)
        res['ResourcesMaxSize'] = max(sizes)
    else:
        res['ResourcesNb'] = 0
        res['ResourcesMeanEntropy'] = 0
        res['ResourcesMinEntropy'] = 0
        res['ResourcesMaxEntropy'] = 0
        res['ResourcesMeanSize'] = 0
        res['ResourcesMinSize'] = 0
        res['ResourcesMaxSize'] = 0

    # Load configuration size
    try:
        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size
    except AttributeError:
        res['LoadConfigurationSize'] = 0


    # Version configuration size
    try:
        version_infos = get_version_info(pe)
        res['VersionInformationSize'] = len(version_infos.keys())
    except AttributeError:
        res['VersionInformationSize'] = 0
    return res

def split_xy_from_df(df, label):
    X = df.drop([label], axis=1).values
    y = df[label].values

    return X, y


def preprocess_for_train(df, encoder_path=None):
    categorical_encoded_df, encoder_dict = encoding_malware_categorical_feature(df)
    numerical_scaled_df = minmax_scale_malware(df, encoder_path)

    if encoder_path != None:
        for name, encoder in encoder_dict.items():
            save(os.path.join(encoder_path,name) + ENCODER_SUFFIX, encoder)

    new_df = pd.concat([categorical_encoded_df, numerical_scaled_df], axis=1)

    return new_df, encoder_dict


def encoding_malware_categorical_feature(df):
    try:
        encoder_dict = dict()
        new_df = pd.DataFrame()
        for feature in categorical_columns:
            feature_df = df[feature].copy()
            unique_value_len = len(feature_df.unique())

            if (unique_value_len < 5) or feature=='Machine':
                encoder = OneHotEncoder(sparse=False).fit(df[[feature]])
                encoded = encoder.transform(df[[feature]])
                #print("OneHotEncoding feature_names : ", encoder.get_feature_names_out())
                encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
                new_df = pd.concat([new_df, encoded_df], axis=1)
                encoder_dict[feature] = encoder

            else:
                encoder = ce.BinaryEncoder(cols=[feature]).fit(df[[feature]])
                encoded = encoder.transform(df[[feature]])
                #print("BinaryEncoding feature_names : ", pd.DataFrame(encoded).columns)
                encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
                new_df = pd.concat([new_df, encoded_df], axis=1)
                encoder_dict[feature] = encoder


        return new_df, encoder_dict

    except Exception as e:
        print("return (pd.DataFrame, Encoder)")
        print("Error :", e)

def minmax_scale_malware(df, save_path=None):
    minmax_scaler = MinMaxScaler()
    minmax_scaler.fit(df[numerical_columns])
    scaled_num = minmax_scaler.transform(df[numerical_columns])

    if save_path != None:
        scaler_save_path = os.path.join(save_path, "mal_minmax_scaler"+SCALER_SUFFIX)
        save(scaler_save_path, minmax_scaler)

    scaled_df = pd.DataFrame(columns=numerical_columns, data=scaled_num)

    return scaled_df

def preprocess_for_inference(df):
    categorical_encoded_df = encoding_cat_feature_for_inference(df)
    numerical_scaled_df = minmax_scale_for_inference(df)

    new_df = pd.concat([categorical_encoded_df, numerical_scaled_df], axis=1)

    return new_df


def encoding_cat_feature_for_inference(df, encoder_dict=None):
    encoder_list = get_full_path(ENCODER_SUFFIX)

    new_df = pd.DataFrame()
    for encoder_path in encoder_list:
        if ('encoder' in encoder_path) and (ENCODER_SUFFIX in encoder_path):
            encoder = load(encoder_path)
            feature_name = encoder_path.split('/')[-1].split('.')[0]
            encoded = encoder.transform(df[[feature_name]])
            #print("\nencoder path : ", encoder_path)
            #print("feature name : ", feature_name)
            #print("get feature names out : ", encoder.get_feature_names_out())
            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
            new_df = pd.concat([new_df, encoded_df], axis=1)

    return new_df


def minmax_scale_for_inference(df):
    scaler_path_list = get_full_path(SCALER_SUFFIX)
    #print("scaler : ", scaler_path_list)

    scaler_path = scaler_path_list[0]
    scaler = load(scaler_path)
    scaled = scaler.transform(df[numerical_columns])

    return pd.DataFrame(columns=numerical_columns, data=scaled)

def label_encoding_multi_class_train(y_class, save_path=None):
    label_encoder = LabelEncoder()
    label_encoder.fit(y_class)

    if save_path != None:
        save(os.path.join(save_path, 'mal_label_encoder'+LABEL_SUFFIX), label_encoder)

    y_encoded = label_encoder.transform(y_class)
    print(np.unique(y_encoded, return_counts=True))

    return y_encoded, label_encoder

def label_encoding_multi_class_inference(y_class):
    label_encoder_path_list = get_full_path(LABEL_SUFFIX)

    label_encoder_path = label_encoder_path_list[0]
    label_encoder = load(label_encoder_path)
    encoded_label = label_encoder.transform(y_class)

    return encoded_label


def load(path):
    encoder = joblib.load(path)
    return encoder

def save(path, encoder):
    joblib.dump(encoder, path)
    #print("Success save : ", encoder_path)


def delete_outlier(df, columns):
    '''
        Example) delete_outlier_df = detect_outliers(delete_outlier_df[delete_outlier_df['label']==1], 'SizeOfCode')
                 delete_outlier_df = pd.concat([x_df[x_df['label']==0], delete_outlier_df])        
    '''

    q1 = df[columns].quantile(0.25)
    q3 = df[columns].quantile(0.75)
    iqr = q3 - q1

    boundary = 1.5*iqr

    index1 = df[df[columns] > q3+boundary].index
    index2 = df[df[columns] < q1-boundary].index

    df = df.drop(index1)
    df = df.drop(index2)

    return df


def oss_under_sampling(X, y, n_neighbors=1, n_seeds_S=1):
    oss_undersample = OneSidedSelection(n_neighbors=1, n_seeds_S=3, random_state=42)
    oss_X, oss_y = oss_undersample.fit_resample(X, y)
    print(pd.DataFrame(oss_y).value_counts())
    return oss_X, oss_y