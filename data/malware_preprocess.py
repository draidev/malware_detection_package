"""
this file extracts the required information of a given file using the library PE 

"""
import pefile
import array
import math
import os
import joblib
import hashlib
import struct # use save pe structure

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import category_encoders as ce

import numpy as np
import pandas as pd
from .data_loader import get_full_path

from imblearn.under_sampling import \
    RandomUnderSampler, TomekLinks, OneSidedSelection, CondensedNearestNeighbour, EditedNearestNeighbours

categorical_columns=['Machine', 'Characteristics', 'MajorLinkerVersion',
                     'MinorLinkerVersion', 'SectionAlignment', 'FileAlignment',
                     'Subsystem', 'MajorOperatingSystemVersion', 'MinorOperatingSystemVersion',
                     'MajorImageVersion', 'MinorImageVersion', 'MajorSubsystemVersion',
                     'MinorSubsystemVersion']

numerical_columns=['SizeOfCode','SizeOfInitializedData', 'SizeOfUninitializedData',
       'AddressOfEntryPoint', 'BaseOfCode', 'BaseOfData', 'ImageBase', 'SizeOfImage',
       'SizeOfHeaders', 'CheckSum', 'DllCharacteristics',
       'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve',
       'SizeOfHeapCommit', 'NumberOfRvaAndSizes', 'SectionsNb',
       'SectionsMeanEntropy', 'SectionsMinEntropy', 'SectionsMaxEntropy',
       'SectionsMeanRawsize', 'SectionsMinRawsize', 'SectionsMaxRawsize',
       'SectionsMeanVirtualsize', 'SectionsMinVirtualsize',
       'SectionMaxVirtualsize', 'ImportsNbDLL', 'ImportsNb',
       'ImportsNbOrdinal', 'ExportNb', 'ResourcesNb', 'ResourcesMeanEntropy',
       'ResourcesMinEntropy', 'ResourcesMaxEntropy', 'ResourcesMeanSize',
       'ResourcesMinSize', 'ResourcesMaxSize', 'LoadConfigurationSize',
       'VersionInformationSize']

ENCODER_SUFFIX = ".enc"
SCALER_SUFFIX = ".scl"
LABEL_SUFFIX = ".lb"

def exe_to_df(exe_file_list:list, processed_list=None, error_list=None):
    df = pd.DataFrame()
    step = 0
    error_cnt = 0
    for exe in exe_file_list:
        try :
            step+=1
            print(step, end=" ")
            pe_dict = extract_infos(exe)
            exe_df = pd.DataFrame.from_dict(pe_dict, orient='index').T
            df = pd.concat([df, exe_df], axis=0)
            if processed_list != None:
                processed_list.append(exe)
        except Exception as e:   
            error_cnt+=1
            print("\nError:", e)
            print(error_cnt, exe)
            if error_list != None:
                error_list.append(exe)
            pass

    return df


def get_entropy(data):
    if len(data) == 0:
        return 0.0
    occurences = array.array('L', [0]*256)
    for x in data:
        occurences[x if isinstance(x, int) else ord(x)] += 1
    entropy = 0
    for x in occurences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x*math.log(p_x, 2)
    return entropy


def get_resources(pe):
    """Extract resources :
    [entropy, size]"""
    resources = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                resources.append([entropy, size])
        except Exception as e:
            return resources
    return resources


def get_version_info(pe):
    """Return version infos"""
    res = {}
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]
    if hasattr(pe, 'VS_FIXEDFILEINFO'):
        res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
        res['os'] = pe.VS_FIXEDFILEINFO.FileOS
        res['type'] = pe.VS_FIXEDFILEINFO.FileType
        res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
        res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
        res['signature'] = pe.VS_FIXEDFILEINFO.Signature
        res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion
    return res


#extract the info for a given file
# args already_pe will get tuple like (hash_key, pe_value)
def extract_infos(fpath=None, already_pe=None, inference=None):
    res = {}
    
    try:
        if fpath != None:
            pe = pefile.PE(fpath)
        else:
            pe = already_pe[1]
    except FileNotFoundError as e:
        print(f"File not found: {fpath}")
    except pefile.PEFormatError as e:
        print(f"PEFormatError: {fpath} does not appear to ba a PE file.")
        print("Error : ", e)
        return

    if fpath!=None and inference==None:
        res['md5'] = file_to_md5(fpath)
    elif already_pe!=None:
        res['md5'] = already_pe[0]

    res['Machine'] = pe.FILE_HEADER.Machine
    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    res['Characteristics'] = pe.FILE_HEADER.Characteristics
    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    try:
        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
    except AttributeError:
        res['BaseOfData'] = 0
    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage
    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders
    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit
    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags
    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes
    
       # Sections
    res['SectionsNb'] = len(pe.sections)
    entropy = list(map(lambda x:x.get_entropy(), pe.sections))
    res['SectionsMeanEntropy'] = sum(entropy)/float(len(entropy))
    res['SectionsMinEntropy'] = min(entropy)
    res['SectionsMaxEntropy'] = max(entropy)
    raw_sizes = list(map(lambda x:x.SizeOfRawData, pe.sections))
    res['SectionsMeanRawsize'] = sum(raw_sizes)/float(len(raw_sizes))
    res['SectionsMinRawsize'] = min(raw_sizes)
    res['SectionsMaxRawsize'] = max(raw_sizes)
    virtual_sizes = list(map(lambda x:x.Misc_VirtualSize, pe.sections))
    res['SectionsMeanVirtualsize'] = sum(virtual_sizes)/float(len(virtual_sizes))
    res['SectionsMinVirtualsize'] = min(virtual_sizes)
    res['SectionMaxVirtualsize'] = max(virtual_sizes)

    #Imports
    try:
        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)
        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
        res['ImportsNb'] = len(imports)
        res['ImportsNbOrdinal'] = len(list(filter(lambda x:x.name is None, imports)))
    except AttributeError:
        res['ImportsNbDLL'] = 0
        res['ImportsNb'] = 0
        res['ImportsNbOrdinal'] = 0

    #Exports
    try:
        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
    except AttributeError:
        # No export
        res['ExportNb'] = 0
    #Resources
    resources= get_resources(pe)
    res['ResourcesNb'] = len(resources)
    
    if len(resources)> 0:
        entropy = list(map(lambda x:x[0], resources))
        res['ResourcesMeanEntropy'] = sum(entropy)/float(len(entropy))
        res['ResourcesMinEntropy'] = min(entropy)
        res['ResourcesMaxEntropy'] = max(entropy)
        sizes = list(map(lambda x:x[1], resources))
        res['ResourcesMeanSize'] = sum(sizes)/float(len(sizes))
        res['ResourcesMinSize'] = min(sizes)
        res['ResourcesMaxSize'] = max(sizes)
    else:
        res['ResourcesNb'] = 0
        res['ResourcesMeanEntropy'] = 0
        res['ResourcesMinEntropy'] = 0
        res['ResourcesMaxEntropy'] = 0
        res['ResourcesMeanSize'] = 0
        res['ResourcesMinSize'] = 0
        res['ResourcesMaxSize'] = 0

    # Load configuration size
    try:
        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size
    except AttributeError:
        res['LoadConfigurationSize'] = 0


    # Version configuration size
    try:
        version_infos = get_version_info(pe)
        res['VersionInformationSize'] = len(version_infos.keys())
    except AttributeError:
        res['VersionInformationSize'] = 0
    return res

def split_xy_from_df(df, label):
    X = df.drop([label], axis=1).values
    y = df[label].values

    return X, y


def preprocess_for_train(df, encoder_path=None):
    categorical_encoded_df, encoder_dict = encoding_malware_categorical_feature(df)
    numerical_scaled_df = minmax_scale_malware(df, encoder_path)

    if encoder_path != None:
        for name, encoder in encoder_dict.items():
            save(os.path.join(encoder_path,name) + ENCODER_SUFFIX, encoder)

    new_df = pd.concat([categorical_encoded_df, numerical_scaled_df], axis=1)

    return new_df, encoder_dict


def encoding_malware_categorical_feature(df):
    try:
        encoder_dict = dict()
        new_df = pd.DataFrame()
        for feature in categorical_columns:
            feature_df = df[feature].copy()
            unique_value_len = len(feature_df.unique())

            if (unique_value_len < 5) or feature=='Machine':
                encoder = OneHotEncoder(sparse=False).fit(df[[feature]])
                encoded = encoder.transform(df[[feature]])
                #print("OneHotEncoding feature_names : ", encoder.get_feature_names_out())
                encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
                new_df = pd.concat([new_df, encoded_df], axis=1)
                encoder_dict[feature] = encoder

            else:
                encoder = ce.BinaryEncoder(cols=[feature]).fit(df[[feature]])
                encoded = encoder.transform(df[[feature]])
                #print("BinaryEncoding feature_names : ", pd.DataFrame(encoded).columns)
                encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
                new_df = pd.concat([new_df, encoded_df], axis=1)
                encoder_dict[feature] = encoder


        return new_df, encoder_dict

    except Exception as e:
        print("return (pd.DataFrame, Encoder)")
        print("Error :", e)

def minmax_scale_malware(df, save_path=None):
    minmax_scaler = MinMaxScaler()
    minmax_scaler.fit(df[numerical_columns])
    scaled_num = minmax_scaler.transform(df[numerical_columns])

    if save_path != None:
        scaler_save_path = os.path.join(save_path, "mal_minmax_scaler"+SCALER_SUFFIX)
        save(scaler_save_path, minmax_scaler)

    scaled_df = pd.DataFrame(columns=numerical_columns, data=scaled_num)

    return scaled_df

def preprocess_for_inference(df, pre_path=None):
    categorical_encoded_df = encoding_cat_feature_for_inference(df, pre_path)
    numerical_scaled_df = minmax_scale_for_inference(df, pre_path)

    new_df = pd.concat([categorical_encoded_df, numerical_scaled_df], axis=1)

    return new_df


def encoding_cat_feature_for_inference(df, pre_path=None, encoder_dict=None):
    #encoder_list = get_full_path(ENCODER_SUFFIX)
    
    ## encoding sequence is important to inference.
    enc_list = ['Machine.enc', 'Characteristics.enc', 'MajorLinkerVersion.enc', 'MinorLinkerVersion.enc', 'SectionAlignment.enc', 'FileAlignment.enc', 'Subsystem.enc', 'MajorOperatingSystemVersion.enc', 'MinorOperatingSystemVersion.enc', 'MajorImageVersion.enc', 'MinorImageVersion.enc', 'MajorSubsystemVersion.enc', 'MinorSubsystemVersion.enc']

    if pre_path == None:
        pre_path = './'

    encoder_list = list()
    for enc in enc_list:
        enc_path = os.path.join(pre_path, enc)
        encoder_list.append(enc_path)
        
    new_df = pd.DataFrame()
    for encoder_path in encoder_list:
        if ('encoder' in encoder_path) and (ENCODER_SUFFIX in encoder_path):
            encoder = load(encoder_path)
            feature_name = encoder_path.split('/')[-1].split('.')[0]
            encoded = encoder.transform(df[[feature_name]])
            #print("\nencoder path : ", encoder_path)
            #print("feature name : ", feature_name)
            #print("get feature names out : ", encoder.get_feature_names_out())
            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())
            new_df = pd.concat([new_df, encoded_df], axis=1)

    return new_df


def minmax_scale_for_inference(df, pre_path=None):
    scaler_path_list = get_full_path(SCALER_SUFFIX, pre_path)
    #print("scaler : ", scaler_path_list)

    scaler_path = scaler_path_list[0]
    scaler = load(scaler_path)
    scaled = scaler.transform(df[numerical_columns])

    return pd.DataFrame(columns=numerical_columns, data=scaled)

def label_encoding_multi_class_train(y_class, save_path=None):
    label_encoder = LabelEncoder()
    label_encoder.fit(y_class)

    if save_path != None:
        save(os.path.join(save_path, 'mal_label_encoder'+LABEL_SUFFIX), label_encoder)

    y_encoded = label_encoder.transform(y_class)
    print(np.unique(y_encoded, return_counts=True))

    return y_encoded, label_encoder

def label_encoding_multi_class_inference(y_class):
    label_encoder_path_list = get_full_path(LABEL_SUFFIX)

    label_encoder_path = label_encoder_path_list[0]
    label_encoder = load(label_encoder_path)
    encoded_label = label_encoder.transform(y_class)

    return encoded_label


def load(path):
    encoder = joblib.load(path)
    return encoder

def save(path, encoder):
    joblib.dump(encoder, path)
    #print("Success save : ", encoder_path)


def delete_outlier(df, columns):
    '''
        Example) delete_outlier_df = detect_outliers(delete_outlier_df[delete_outlier_df['label']==1], 'SizeOfCode')
                 delete_outlier_df = pd.concat([x_df[x_df['label']==0], delete_outlier_df])        
    '''

    q1 = df[columns].quantile(0.25)
    q3 = df[columns].quantile(0.75)
    iqr = q3 - q1

    boundary = 1.5*iqr

    index1 = df[df[columns] > q3+boundary].index
    index2 = df[df[columns] < q1-boundary].index

    df = df.drop(index1)
    df = df.drop(index2)

    return df


def oss_under_sampling(X, y, n_neighbors=1, n_seeds_S=1):
    oss_undersample = OneSidedSelection(n_neighbors=1, n_seeds_S=3, random_state=42)
    oss_X, oss_y = oss_undersample.fit_resample(X, y)
    print(pd.DataFrame(oss_y).value_counts())
    return oss_X, oss_y


# give path list of exe file
def exe_list_to_df(exe_file_list):
    df = pd.DataFrame()
    step = 0
    error_cnt = 0
    for exe in exe_file_list:
        try :
            step+=1
            print(step, end=" ")
            extracted_pe_dict = extract_infos(exe)
            exe_df = pd.DataFrame.from_dict(extracted_pe_dict, orient='index').T
            df = pd.concat([df, exe_df], axis=0)
        except Exception as e:   
            error_cnt+=1
            print("\nError:", e)
            print(error_cnt, exe)
            pass

    return df

# {hash 값 : pe 구조체} 받아서 dataframe 생성 
def pe_dict_to_df(pe_dict):
    df = pd.DataFrame()
    step = 0
    error_cnt = 0
    for key in pe_dict.keys():
        try :
            step+=1
            print(step, end=" ")
            extracted_pe_dict = extract_infos(already_pe=(key, pe_dict[key]))
            exe_df = pd.DataFrame.from_dict(extracted_pe_dict, orient='index').T
            df = pd.concat([df, exe_df], axis=0)
        except Exception as e:   
            error_cnt+=1
            print("\nError:", e)
            print(error_cnt, key)
            pass

    return df.reset_index(drop=True)

    


def file_to_md5(filepath):
    hash_md5 = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)

    return hash_md5.hexdigest()

def exe_list_to_md5(exe_file_list):
    md5_list = []
    for file in exe_file_list:
        md5 = file_to_md5(file)
        md5_list.append(md5)

    return md5_list    


def save_multiple_pe_data(pe_data, filename):
    if os.path.exists(filename):
         with open(filename, 'ab') as f:
            f.write(struct.pack('<I', len(pe_data)))
            f.write(pe_data)
    else:
        with open(filename, 'wb') as f:
            f.write(struct.pack('<I', len(pe_data)))
            f.write(pe_data)
                
def load_multiple_pe_data(filename):
    pe_data_list = list()
    with open(filename, 'rb') as f:
        while True:
            size_bytes = f.read(4)
            if not size_bytes:
                break
            size = struct.unpack('<I', size_bytes)[0]
            pe_data = f.read(size)
            pe_data_list.append(pe_data)

    return pe_data_list

# if file name like hash and save hash value from filename
def save_hash_list_from_hash_file_name(exe_file_list, save_path):
    hash_list = list()
    for f in exe_file_list:  #  파일 리스트 수정 필요
        hash_list.append(f.split('/')[-1].split('.')[0])

    pd.DataFrame(hash_list).to_csv(save_path, index=False)


def save_hash_pe_data_from_exe_list(exe_file_list, filename=None, save=None, hash_save_path=None, error_save_path=None):
    pe_dict = dict()
    old_df = pd.DataFrame()
    new_df = pd.DataFrame()
    step = 0

    new_df.to_csv(hash_save_path, index=False)
    # if hash_save_path != None:
    #   save_hash_list_from_hash_file_name(exe_file_list, hash_save_path)
    #   print('hash save success!!')

    error_list=list()
    
    for exe in exe_file_list:
        try:
            step += 1
            print(step, end=" ")
            print("path:", exe)
            pe = pefile.PE(exe)
            md5 = file_to_md5(exe)
            pe_dict[md5] = pe

            if save == True:
                save_multiple_pe_data(pe.__data__, filename)

                if step > 1:
                    old_df = pd.read_csv(hash_save_path)

                md5_df = pd.DataFrame([md5], columns=['md5'])
                new_df = pd.concat([old_df, md5_df], axis=0)
                new_df.to_csv(hash_save_path, index=False)
                print(f"{step} md5 :", md5)
                print()
                
            pe.close()
        except Exception as e:
            print('\n', e)
            error_list.append(exe.split('/')[-1].split('.')[0])
            pass
    if error_save_path != None:
        pd.DataFrame(error_list).to_csv(error_save_path, index=False)
            
    return pe_dict, new_df

def save_error_hash(exe_file_list, error_save_path):
    step = 0
    error_list = list()

    for exe in exe_file_list:
        try:
            print(step, end=" ")
            print("path:", exe)
            step += 1
            pe = pefile.PE(exe)
            
            pe.close()
        
        except Exception as e:
            print('\n', e)
            error_list.append(exe.split('/')[-1].split('.')[0])
            pass

    pd.DataFrame(error_list).to_csv(error_save_path, index=False)

def make_pe_header_df(md5_list, pe_data_list):
    df_dict = dict()
    new_df = pd.DataFrame(columns=['md5', 'pe_header', 'label'])
    for i, md5 in enumerate(md5_list):
        print(i, md5, end=' ')
        df_dict['md5'] = md5
        df_dict['pe_header'] = pefile.PE(data=pe_data_list[i])
        df_dict['label'] = df2[df2['md5']==md5]['label'].values[0]
        temp_df = pd.DataFrame.from_dict(df_dict, orient='index').T
        new_df = pd.concat([new_df, temp_df], axis=0)

    return new_df


# concat pe_data file
def extend_data_save(save_path, *args):
    data_list = []
    for x in args:
        print(x)
        data_list.extend(load_multiple_pe_data(x))

    print(len(data_list))
    
    step = 0
    for data in data_list:
        print(step, end=' ')
        step += 1
        save_multiple_pe_data(data, save_path)

def save_exefile_md5_pe_to_df(exe_file_list):
    pe_list = list()
    md5_hash_list = list()
    step = 0
    for exe in exe_file_list:
        try:
            step += 1
            print(step, end=' ')
            pe = pefile.PE(exe)
            md5_hash = file_to_md5(exe)

            pe_list.append(pe.__data__)
            md5_hash_list.append(md5_hash)

        except Exception as e:
            print('\n', step, e)

    df = pd.DataFrame({'md5':md5_hash_list, 'pe_header':pe_list})
